{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup emulating scikit-learn object but just using `numpy` and `pandas` packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages & setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.random.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Random Data Generated>\n",
      "(Sample Size, Features) = (1000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>8.14</td>\n",
       "      <td>3.68</td>\n",
       "      <td>7.18</td>\n",
       "      <td>8.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.74</td>\n",
       "      <td>3.23</td>\n",
       "      <td>8.84</td>\n",
       "      <td>6.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.13</td>\n",
       "      <td>9.13</td>\n",
       "      <td>9.84</td>\n",
       "      <td>6.34</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.36</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.77</td>\n",
       "      <td>8.73</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.43</td>\n",
       "      <td>7.39</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.41</td>\n",
       "      <td>3.16</td>\n",
       "      <td>8.47</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.95</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>6.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.28</td>\n",
       "      <td>6.35</td>\n",
       "      <td>5.94</td>\n",
       "      <td>8.62</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0       1.0      9.44      0.86      2.32      1.54      4.88      1.44   \n",
       "1       1.0      8.74      3.23      8.84      6.34      1.66      1.13   \n",
       "2       1.0      8.36      4.18      1.77      8.73      0.56      0.59   \n",
       "3       1.0      7.41      3.16      8.47      8.58      0.28      3.54   \n",
       "4       1.0      3.36      6.22      2.38      5.09      0.28      6.35   \n",
       "\n",
       "   feature7  feature8  feature9  feature10  \n",
       "0      8.14      3.68      7.18       8.53  \n",
       "1      9.13      9.84      6.34       4.89  \n",
       "2      1.43      7.39      2.06       2.84  \n",
       "3      3.95     10.00      3.62       2.52  \n",
       "4      5.94      8.62      3.92       3.25  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the random data\n",
    "has_intercept = True\n",
    "observations = 1000\n",
    "features = 10\n",
    "\n",
    "rand_data = np.around(10 * np.random.rand(observations, features), 2) # 1 <= data < 10\n",
    "X = pd.DataFrame(rand_data, columns=[f'feature{i}' for i in range(1, features + 1)])\n",
    "if has_intercept:\n",
    "    X.insert(0, 'feature0', np.ones(observations)) # Set dummy column in data\n",
    "y = pd.Series(np.random.randint(2, size=observations))\n",
    "theta = pd.Series(np.random.rand(len(X.columns)))\n",
    "\n",
    "print(f'<Random Data Generated>\\n(Sample Size, Features) = {observations, features}')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup metaclasses\n",
    "class Estimator(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, normalize=False):\n",
    "        self._normalize = normalize\n",
    "        self._coefs = np.array([])\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, design_matrix, labels):\n",
    "        pass\n",
    "    \n",
    "    def score(self, design_matrix, labels):\n",
    "        pass\n",
    "    \n",
    "class Predictor(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, design_matrix):\n",
    "        pass\n",
    "\n",
    "# Setup helper Hypothesis class\n",
    "class _Hypothesis:\n",
    "    \n",
    "    models = {\n",
    "        'linear': lambda X, theta: np.dot(X, theta),\n",
    "        'logistic': lambda X, theta: 1 / (1 + np.exp(- np.dot(X, theta)))\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def apply_model(self, design_matrix, parameters):\n",
    "        try:\n",
    "            hypothesis_func = models[self.model]\n",
    "            return hypothesis_func(design_matrix, parameters)\n",
    "        except:\n",
    "            raise KeyError(f'Please use one of the follwing: {tuple(models.keys())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4]\n",
      "Whoops, mispelled the hypothesis function!\n"
     ]
    }
   ],
   "source": [
    "# Test the Hypothesis class\n",
    "X = np.array([[1, 2], [-2, 3]])\n",
    "theta = np.array([1, 2])\n",
    "h = _Hypothesis('linear')\n",
    "print(h.apply_model(X, theta))\n",
    "e = _Hypothesis('mispelled')\n",
    "try:\n",
    "    e.apply_model(X, theta)\n",
    "except KeyError:\n",
    "    print('Whoops, mispelled the hypothesis function!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Estimator, Predictor):\n",
    "    \"\"\"\n",
    "    This is a class to mimic the functionality from sklearn.linear_model.LinearRegression.\n",
    "    \n",
    "    Attributes:\n",
    "        _coef (Numpy.Array): model coefficients (Thetas)\n",
    "        _intercept (Float): model interecpt term (Theta0); is None if fit_intercept=False \n",
    "        _fit_intercept (Boolean): flag to determine if model should have intercept term fitted (default=True)\n",
    "        _normalize (Boolean): flag to determine if data should be normalized (default=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cost_function():\n",
    "        cost_functions = {\n",
    "            'mse': lambda X, y, theta, model='linear': np.mean((y - _Hypothesis(model).apply_model(X, theta)) ** 2)\n",
    "        }\n",
    "        return cost_functions\n",
    "    \n",
    "    def __init__(self, normalize=False, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        Constructor for LinearRegression class.\n",
    "        \n",
    "        Parameters:\n",
    "            fit_intercept (Boolean): flag to determine if model should have intercept term fitted (default=True)\n",
    "            normalize (Boolean): flag to determine if data should be normalized (default=False)\n",
    "        \"\"\"\n",
    "        super().__init__(normalize)\n",
    "        self._fit_intercept = fit_intercept\n",
    "        self._intercept = float() if self._fit_intercept else None\n",
    "    \n",
    "    def fit(self, design_matrix, labels):\n",
    "        # TODO fix logic to see whether features should handle dummy column here or in utility function \n",
    "        \"\"\"\n",
    "        Estimates the parameters (theta) of the linear regression model using Gradient Descent.\n",
    "        \n",
    "        Parameters:\n",
    "            features (pandas.DataFrame): design matrix (X)\n",
    "            labels (pandas.Series): target array (y) \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if self._normalize:\n",
    "            standardize = lambda arr: (arr - np.mean(arr)) / np.std(arr)\n",
    "            for col in features.columns:\n",
    "                design_matrix[col] = standardize(design_matrix[col])\n",
    "            labels = standardize(labels)\n",
    "        num_of_features = len(design_matrix.columns)\n",
    "        degrees_freedom = num_of_features + 1 if self._fit_intercept else num_of_features\n",
    "        initial_params = np.random.rand(degrees_freedom)\n",
    "        fitted_params = gradient_descent(design_matrix, labels, initial_params)\n",
    "        self._coefs = fitted_params[1:]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, design_matrix):\n",
    "        \"\"\"\n",
    "        Constructs prediction array (y hat) using linear_model utilityy function.\n",
    "        \n",
    "        Parameters:\n",
    "            design_matrix (pandas.DataFrame): design matrix (X)\n",
    "        Returns:\n",
    "            pandas.Series: an array of predicted labels\n",
    "        \"\"\"\n",
    "        has_intercept = self._fit_intercept\n",
    "        hyp = _Hypothesis('linear')\n",
    "        # Slices parameter vector based on self._fit_intercept value (0/False or 1/True)\n",
    "        predictions = hyp.apply_model(design_matrix, self._coefs[has_intercept:])\n",
    "        bias = self._intercept\n",
    "        y_hats = predictions + bias if has_intercept else predictions\n",
    "        return y_hats\n",
    "    \n",
    "    def score(self, design_matrix, labels):\n",
    "        \"\"\"\n",
    "        Calculates R^2 value from data (X) & labels (y)\n",
    "        \n",
    "        Parameters:\n",
    "            design_matrix (pandas.DataFrame): design matrix (X)\n",
    "            labels (pandas.Series): target array (y) \n",
    "        Returns:\n",
    "            float: R-squared value of model; value lies in [0, 1]\n",
    "        \"\"\"\n",
    "        mean_squared_error = type(self).get_cost_function()['mse']\n",
    "        mse = mean_squared_error(design_matrix, labels, self._coefs)\n",
    "        r_squared = 1 - mse / np.var(labels)\n",
    "        return r_squared\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Obtains paramters array of Linear Regression model.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.Array: parameters of model (theta)\n",
    "        \"\"\"\n",
    "        return self._coefs\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"\n",
    "        Sets paramters array of Linear Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "            parameters (numpy.Array): parameters of model (theta)\n",
    "        \"\"\"\n",
    "        self._coefs = parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMetrics:\n",
    "    \n",
    "    def sse(self):\n",
    "        '''returns sum of squared errors (model vs actual)'''\n",
    "        squared_errors = (self.target - self.predict(self.data)) ** 2\n",
    "        self.sq_error_ = np.sum(squared_errors)\n",
    "        return self.sq_error_\n",
    "        \n",
    "    def sst(self):\n",
    "        '''returns total sum of squared errors (actual vs avg(actual))'''\n",
    "        avg_y = np.mean(self.target)\n",
    "        squared_errors = (self.target - avg_y) ** 2\n",
    "        self.sst_ = np.sum(squared_errors)\n",
    "        return self.sst_\n",
    "    \n",
    "    def r_squared(self):\n",
    "        '''returns calculated value of r^2'''\n",
    "        self.r_sq_ = 1 - self.sse()/self.sst()\n",
    "        return self.r_sq_\n",
    "    \n",
    "    def adj_r_squared(self):\n",
    "        '''returns calculated value of adjusted r^2'''\n",
    "        self.adj_r_sq_ = 1 - (self.sse()/self._dfe) / (self.sst()/self._dft)\n",
    "        return self.adj_r_sq_\n",
    "    \n",
    "    def mse(self):\n",
    "        '''returns calculated value of mse'''\n",
    "        self.mse_ = np.mean( (self.predict(self.data) - self.target) ** 2 )\n",
    "        return self.mse_\n",
    "    \n",
    "    def pretty_print_stats(self):\n",
    "        '''returns report of statistics for a given model object'''\n",
    "        items = ( ('sse:', self.sse()), ('sst:', self.sst()), \n",
    "                 ('mse:', self.mse()), ('r^2:', self.r_squared()), \n",
    "                  ('adj_r^2:', self.adj_r_squared()))\n",
    "        for item in items:\n",
    "            print('{0:8} {1:.4f}'.format(item[0], item[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegressionWithInheritance(ModifiedMetrics):\n",
    "    \n",
    "    \n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self._fit_intercept = fit_intercept\n",
    "          \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model coefficients.\n",
    "\n",
    "        Arguments:\n",
    "        X: 1D or 2D numpy array \n",
    "        y: 1D numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        # training data & ground truth data\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        \n",
    "        # degrees of freedom population dep. variable variance \n",
    "        self._dft = X.shape[0] - 1  \n",
    "        # degrees of freedom population error variance\n",
    "        self._dfe = X.shape[0] - X.shape[1] - 1\n",
    "        \n",
    "        # check if X is 1D or 2D array\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "            \n",
    "        # add bias if fit_intercept\n",
    "        if self._fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # closed form solution\n",
    "        xTx = np.dot(X.T, X)\n",
    "        inverse_xTx = np.linalg.inv(xTx)\n",
    "        xTy = np.dot(X.T, y)\n",
    "        coef = np.dot(inverse_xTx, xTy)\n",
    "        \n",
    "        # set attributes\n",
    "        if self._fit_intercept:\n",
    "            self.intercept_ = coef[0]\n",
    "            self.coef_ = coef[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "            self.coef_ = coef\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Output model prediction.\n",
    "\n",
    "        Arguments:\n",
    "        X: 1D or 2D numpy array \n",
    "        \"\"\"\n",
    "        # check if X is 1D or 2D array\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1,1) \n",
    "        return self.intercept_ + np.dot(X, self.coef_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
